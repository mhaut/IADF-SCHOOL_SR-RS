{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Image Prior for Super-Resolution\n",
    "\n",
    "Deep Image Prior (DIP) can be applied to the problem of super-resolution, where the goal is to upscale a low-resolution (LR) image to a higher resolution (HR) using a neural network. Unlike traditional methods that require pre-trained models, DIP uses a randomly initialized network and optimizes it to reconstruct the high-resolution image.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Set up the environment\n",
    "2. Define the neural network architecture\n",
    "3. Load and preprocess images\n",
    "4. Train the network for super-resolution\n",
    "5. Evaluate and display the results\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Neural Network\n",
    "\n",
    "We will use a convolutional neural network for super-resolution. The network consists of several convolutional layers with ReLU activations.\n",
    "\n",
    "Let's define the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DIPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DIPNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, padding=2),  # Input: 3xH x W\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 3, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=4, mode='bilinear', align_corners=False)  # Up-sample to match HR image size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Preprocess Images\n",
    "\n",
    "We need to load the high-resolution image and create a low-resolution version by downsampling. Then, we'll prepare these images for training.\n",
    "\n",
    "Let's load and preprocess the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, size=(256, 256)):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = image.resize(size, Image.BICUBIC)\n",
    "    transform = transforms.ToTensor()\n",
    "    return transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "def downsample(image, scale_factor):\n",
    "    \"\"\"\n",
    "    Downsample an image by a given scale factor.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: The image tensor to downsample.\n",
    "    - scale_factor: The factor by which to downsample.\n",
    "    \n",
    "    Returns:\n",
    "    - The downsampled image tensor.\n",
    "    \"\"\"\n",
    "    downsampled = torch.nn.functional.interpolate(image, scale_factor=1/scale_factor, mode='bilinear', align_corners=False)\n",
    "    return downsampled\n",
    "\n",
    "def add_noise(image, noise_level=0.1):\n",
    "    noise = noise_level * torch.randn_like(image)\n",
    "    return image + noise\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = 'images/image_hr.png'  # Replace with your image path\n",
    "hr_image = load_image(image_path)\n",
    "\n",
    "# Create a low-resolution image by downsampling\n",
    "lr_image = downsample(hr_image, scale_factor=4)\n",
    "noisy_lr_image = add_noise(lr_image, noise_level=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_images(original, lr, lrnoisy):\n",
    "    \"\"\"\n",
    "    Display the original and noisy images side by side.\n",
    "    \n",
    "    Parameters:\n",
    "    - original: The original high-resolution image tensor.\n",
    "    - noisy: The noisy image tensor.\n",
    "    \"\"\"\n",
    "    # Convert tensors to numpy arrays and transpose dimensions for plotting\n",
    "    original = original.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    lr = lr.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    lrnoisy = lrnoisy.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    \n",
    "    # Plot the images\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 6), gridspec_kw={'width_ratios': [1, 0.5, 0.5]})\n",
    "    axs[0].imshow(original)\n",
    "    axs[0].set_title('Original Image')\n",
    "    axs[0].axis('off')\n",
    "    \n",
    "    axs[1].imshow(lr)\n",
    "    axs[1].set_title('Low Res. Image')\n",
    "    axs[1].axis('off')\n",
    "    \n",
    "    axs[2].imshow(lrnoisy)\n",
    "    axs[2].set_title('Noisy Low Res. Image')\n",
    "    axs[2].axis('off') \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Display the images\n",
    "display_images(hr_image, lr_image, noisy_lr_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Network for Super-Resolution\n",
    "\n",
    "We will train the network to upscale the low-resolution image to its high-resolution counterpart by minimizing the Mean Squared Error (MSE) between the restored and original high-resolution images.\n",
    "\n",
    "Let's define the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, noisy_lr_image, hr_image, num_epochs=2000, learning_rate=0.001):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        restored_image = model(noisy_lr_image)\n",
    "        # Compute loss\n",
    "        loss = criterion(restored_image, hr_image)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize and train the model\n",
    "model = DIPNet().to(device)\n",
    "trained_model = train(model, noisy_lr_image, hr_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate and Display the Results\n",
    "\n",
    "After training, we will use the model to upscale the low-resolution image and visualize the results. This will help us assess how well the network has restored the image.\n",
    "\n",
    "Let's visualize the original, low-resolution, and restored images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(original, lr, restored):\n",
    "    \"\"\"\n",
    "    Display the original high-resolution image, low-resolution image, and the restored image side by side.\n",
    "    \n",
    "    Parameters:\n",
    "    - original: The original high-resolution image tensor.\n",
    "    - lr: The low-resolution image tensor.\n",
    "    - restored: The restored high-resolution image tensor.\n",
    "    \"\"\"\n",
    "    original = original.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    lr = lr.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    restored = restored.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5), gridspec_kw={'width_ratios': [1, 0.5, 1]})\n",
    "    axs[0].imshow(original)\n",
    "    axs[0].set_title('Original Image')\n",
    "    axs[0].axis('off')\n",
    "    \n",
    "    axs[1].imshow(lr)\n",
    "    axs[1].set_title('Low Res Image')\n",
    "    axs[1].axis('off')\n",
    "    \n",
    "    axs[2].imshow(restored)\n",
    "    axs[2].set_title('Restored Image')\n",
    "    axs[2].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Restore the low-resolution image\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    restored_image = model(noisy_lr_image)\n",
    "\n",
    "# Display the images\n",
    "display_images(hr_image, lr_image, restored_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we implemented Deep Image Prior (DIP) for super-resolution. We defined a simple neural network, prepared the data, trained the network to upscale low-resolution images, and visualized the results.\n",
    "\n",
    "Feel free to modify the network architecture, training parameters, or test with different images to explore the capabilities of Deep Image Prior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arqesp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
